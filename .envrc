#!/usr/bin/env bash
# set -ueo pipefail

#################################################################################
# .envrc file to support direnv to expand shell to load and unload environment
# variables depending on the current directory.
# using set -ueo pipefail to produce a failure return code if any command fails
#################################################################################
BASE_SPARK_INSTALLATION_DIR="/Users/${USER}/local_spark_setup"
export BASE_SPARK_INSTALLATION_DIR

# set AWS default region if not defined already
if [[ -z ${AWS_DEFAULT_REGION+x} ]]; then
  AWS_DEFAULT_REGION="us-west-2"
  export AWS_DEFAULT_REGION
fi

# set 
PYTHONPATH="$(pwd)"/src/python
export PYTHONPATH

# add module and Python 3.7 path to PATH environment variable
# this can be different based on Python installation on the host
PATH_add "$(pwd)"/src/python/.venv/bin
# PATH_add /usr/local/Cellar/python@3.7/3.7.10_3/bin

# set PIPENV environment variables
# reference: https://docs.pipenv.org/advanced/
PIPENV_PIPFILE=src/python/.pipfile
export PIPENV_PIPFILE


# disable terminal spinner, for cleaner logs
export PIPENV_NOSPIN=True

# use fancy shell mode
export PIPENV_SHELL_FANCY=True

# used to create a Pipfile.lock, which declares all dependencies (and sub-dependencies) of project, 
# their latest available versions, and the current hashes for the downloaded files. 
# This ensures repeatable, and most importantly deterministic, builds.
export PIPENV_SKIP_LOCK=True

# setting default Python version, can be changed as needed
# export PIPENV_DEFAULT_PYTHON_VERSION="3.7"

# the virtualenv to be created in the .venv directory next to the Pipfile file
export PIPENV_VENV_IN_PROJECT="enabled"
export PIPENV_IGNORE_VIRTUALENVS=1

# set and create Pylint output directory 
PYLINTHOME="$(pwd)"/.output/pylint
export PYLINTHOME
mkdir -p "${PYLINTHOME}"

JAVA_HOME="$(brew --prefix openjdk@8)"
export JAVA_HOME
export SPARK_HOME=${BASE_SPARK_INSTALLATION_DIR}/spark-2.4.3-bin-spark-2.4.3-bin-hadoop2.8

# I also need to check if I can just zip awsglue directory right after git clone or have to run ./bin/gluepyspark once
# SPARK and GLUE
if [[ -d "src/python/.venv" ]]; then
  PYSPARK_PYTHON="$(pipenv --venv)"/bin/python
  export PYSPARK_PYTHON
fi
PYSPARK_PYTHON_DRIVER="${PYSPARK_PYTHON}"
export PYSPARK_PYTHON_DRIVER
PYTHONPATH="$SPARK_HOME/python/:$PYTHONPATH"
PYTHONPATH=`ls $SPARK_HOME/python/lib/py4j-*-src.zip`:${BASE_SPARK_INSTALLATION_DIR}/aws-glue-libs/PyGlue.zip:"$PYTHONPATH"
export PYTHONPATH
